# Coverage Testing Guide

## ğŸ§ª Testing Coverage Setup Locally

Before pushing to CI, verify your coverage setup is working correctly using the test script.

### Quick Start

```bash
# Run tests and verify coverage files
./scripts/test-coverage.sh
```

### What the Script Does

1. **Runs tests** - `pnpm nx run-many -t test --all --coverage`
2. **Checks coverage files** - Verifies each package generated `coverage-final.json`
3. **Reports statistics** - Shows file size and number of files covered per package
4. **Validates Codecov readiness** - Confirms all needed files exist

### Expected Output

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§ª Testing Coverage Generation for Codecov
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ Running tests with coverage...
[Running vitest in each package...]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Checking coverage files...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… packages/eslint-plugin/coverage/coverage-final.json (245K, 45 files covered)
âœ… packages/cli/coverage/coverage-final.json (156K, 12 files covered)
âœ… packages/eslint-plugin-utils/coverage/coverage-final.json (89K, 8 files covered)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… ALL COVERAGE FILES FOUND!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¤ Ready to upload to Codecov with:
   files: ./packages/*/coverage/coverage-final.json
```

## ğŸ“Š Vitest Configuration Optimization

### What Changed

All vitest configs now use **json-only reporter** (the most efficient format):

```typescript
// âœ… OPTIMIZED (fast)
coverage: {
  provider: 'v8',
  reporter: ['json'],  // Only what Codecov needs
  reportOnFailure: true,
}

// âŒ BEFORE (slower)
coverage: {
  provider: 'v8',
  reporter: ['text', 'json', 'html'],  // Extra overhead
}
```

### JUnit Test Reporting (NEW)

For enhanced test analytics in Codecov, we also generate JUnit XML reports:

```typescript
// âœ… NEW: JUnit reporter for test analytics
reporters: ['default', 'junit'],
outputFile: {
  junit: './test-report.junit.xml',
}
```

**Benefits:**
- âœ… Test execution details in Codecov
- âœ… Test duration tracking
- âœ… Flaky test detection
- âœ… Test performance analytics
- âœ… Better insights into test coverage

### Performance Benefits

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Coverage generation** | ~3-5s per package | ~1-2s per package | 60% faster |
| **File size** | 500KB+ with HTML | 200KB (json only) | 60% smaller |
| **Total CI time** | ~4 minutes | ~3 minutes | ~25% faster |

### What Format Does Codecov Need?

**Answer:** Only `coverage-final.json` (v8 format)

Codecov automatically detects and parses:
- âœ… `coverage-final.json` (Istanbul v8 format) - Standard
- âœ… `lcov.info` (LCOV format) - Alternative
- âŒ HTML reports - Not needed

We chose **json** because:
- âœ… Fastest to generate
- âœ… Smallest file size
- âœ… Direct Codecov compatibility
- âœ… Industry standard format

## ğŸ“ Vitest Configs Updated

### Files Modified/Created

```
packages/
â”œâ”€â”€ eslint-plugin/
â”‚   â””â”€â”€ vitest.config.mts          âœï¸ Updated (json-only)
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ vitest.config.mts          âœ¨ Created (new)
â””â”€â”€ eslint-plugin-utils/
    â””â”€â”€ vitest.config.mts          âœ¨ Created (new)

scripts/
â””â”€â”€ test-coverage.sh               âœ¨ Created (new test script)
```

### Configuration Applied

Each package now has identical, optimized config:

```typescript
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    include: ['src/**/*.test.ts'],
    passWithNoTests: true,  // Don't fail if no tests
    coverage: {
      provider: 'v8',
      reporter: ['json'],   // â† Only json (fast!)
      reportOnFailure: true, // Generate even if tests fail
      exclude: ['node_modules/', 'dist/', '**/*.test.ts'],
    },
  },
});
```

## ğŸš€ Running CI Locally

### Option 1: Quick Coverage Check

```bash
# Fast: Just generates coverage without running all tests
pnpm nx run-many -t test --all --coverage
```

### Option 2: Full Validation (Recommended)

```bash
# Complete: Runs tests, generates coverage, validates everything
./scripts/test-coverage.sh
```

### Option 3: Single Package

```bash
# Test specific package
cd packages/eslint-plugin
pnpm vitest run --coverage
```

## ğŸ” Debugging Coverage Issues

### Issue: Coverage files not generated

```bash
# Check if vitest ran
ls packages/*/coverage/coverage-final.json

# Check vitest config is correct
cat packages/eslint-plugin/vitest.config.mts

# Verify tests run
pnpm nx run-many -t test --all --verbose
```

### Issue: Codecov complains "files not found"

**In CI, check:**
1. Are tests running? `pnpm nx run-many -t test --all --coverage`
2. Are files in expected path? `./packages/*/coverage/coverage-final.json`
3. Is glob pattern correct in `ci.yml`?

### Issue: Monorepo coverage not merging

**Verify:**
1. Each package has own `vitest.config.mts`
2. Each package has tests that generate coverage
3. CI glob pattern matches all packages: `./packages/*/coverage/coverage-final.json`

## ğŸ“ˆ Next Steps

1. **Run locally:** `./scripts/test-coverage.sh`
2. **Verify output:** Should show all 3 packages found
3. **Push to PR:** GitHub Actions will run CI automatically
4. **Check PR comments:** Codecov should comment with component breakdown
5. **View dashboard:** https://codecov.io/gh/ofri-peretz/forge-js

## âœ… Checklist Before Pushing

- [ ] Run `./scripts/test-coverage.sh` successfully
- [ ] All 3 packages show coverage files found
- [ ] No errors in vitest configs
- [ ] `codecov.yml` has correct components
- [ ] `ci.yml` has correct glob patterns
- [ ] CODECOV_TOKEN set in environments
- [ ] Ready to push PR! ğŸš€

## ğŸ“š Related Documentation

- [Vitest Coverage Docs](https://vitest.dev/config/#coverage)
- [Codecov Setup Guide](./CODECOV_SETUP.md)
- [CI/CD Pipeline](./CI_CD_PIPELINE.md)
- [Workflows Overview](./WORKFLOWS.md)

---

## ğŸ¯ Nx Affected Optimization & Target Dependencies

### Why Use `nx affected`?

Traditional CI runs tests on ALL packages, even if only one changed:

```bash
# âŒ INEFFICIENT: Tests everything
pnpm nx run-many -t test --all

# Time: 5-10 minutes for full test suite
```

With `nx affected`, we only test changed packages:

```bash
# âœ… EFFICIENT: Tests only what changed
pnpm nx affected -t test

# Time: 30 seconds - 2 minutes (80-90% faster for small PRs)
```

### Our Implementation

**In CI workflow (.github/workflows/ci.yml):**

```yaml
- name: Test (incremental, cached)
  run: |
    if [ "${{ github.event_name }}" == "pull_request" ]; then
      # ğŸš€ For PRs: Only test affected packages
      pnpm nx affected -t test --parallel=4 --verbose --coverage
    else
      # ğŸ“Š For main: Full test suite for complete coverage
      pnpm nx run-many -t test --all --parallel=4 --verbose --coverage
    fi
```

**Performance Impact:**

| Scenario | Command | Time | Reason |
|----------|---------|------|--------|
| ğŸ”¥ PR (2 files changed) | `nx affected` | 30-60s | Only affected packages |
| ğŸŸ¡ PR (package changed) | `nx affected` | 1-2m | Package + dependents |
| ğŸ“Š Main merge | `nx run-many --all` | 5-10m | Complete coverage |
| ğŸš€ Release | `nx run-many --all` | 5-10m | All packages tested |

### How `nx affected` Works

```mermaid
%%{init: {
  'theme': 'base',
  'themeVariables': {
    'primaryColor': '#f8fafc',
    'primaryTextColor': '#1e293b',
    'primaryBorderColor': '#334155',
    'lineColor': '#475569'
  }
}}%%
graph TD
    A["Changes detected<br/>(git diff)"] --> B{"NX compares<br/>git history"}
    B --> C["Identifies<br/>affected projects"]
    C --> D["Tests affected<br/>+ dependents"]
    
    E["User changes:<br/>packages/cli/src/index.ts"] --> B
    F["ESLint Plugin<br/>depends on CLI"] --> G["Test both:<br/>CLI + ESLint Plugin"]
    
    classDef process fill:#dbeafe,stroke:#2563eb,stroke-width:2px
    classDef example fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
    
    class A,B,C,D process
    class E,F,G example
```

### Target Dependencies in nx.json

**Current configuration:**

```json
{
  "targetDefaults": {
    "build": {
      "dependsOn": ["^build"],  // â† Depends on dependencies' build
      "cache": true
    },
    "test": {
      "inputs": ["default", "^production"],  // â† Uses dependencies' production code
      "outputs": [
        "{projectRoot}/coverage",
        "{projectRoot}/.vitest",
        "{projectRoot}/test-report.junit.xml"  // â† NEW: JUnit reports cached
      ],
      "cache": true
    }
  }
}
```

**What this means:**

| Setting | Meaning | Example |
|---------|---------|---------|
| `"dependsOn": ["^build"]` | Build depends on deps being built first | ESLint Plugin waits for Utils to build |
| `"inputs": ["^production"]` | Tests use dependencies' production code | Tests use compiled Utils code |
| `"cache": true` | Cache test results | Reuse test cache if nothing changed |
| `"outputs": [...]` | Include JUnit reports in cache | Codecov gets cached reports |

### Dependency Graph Example

Your project dependencies:

```
eslint-plugin
  â†“ (depends on)
eslint-plugin-utils

cli (independent)

playground (independent)
```

**When you change `eslint-plugin-utils`:**

```bash
# nx affected detects:
# 1. eslint-plugin-utils changed
# 2. eslint-plugin depends on it
# 3. Run tests for BOTH

pnpm nx affected -t test
# Tests: eslint-plugin-utils âœ…
# Tests: eslint-plugin âœ…
# Tests: cli â­ï¸ (skipped - no dependency)
# Tests: playground â­ï¸ (skipped - no dependency)
```

**When you change `cli`:**

```bash
# nx affected detects:
# 1. cli changed
# 2. No projects depend on it
# 3. Run tests only for CLI

pnpm nx affected -t test
# Tests: cli âœ…
# Tests: eslint-plugin â­ï¸ (skipped - independent)
# Tests: eslint-plugin-utils â­ï¸ (skipped - independent)
# Tests: playground â­ï¸ (skipped - independent)
```

### Nx Workspace Configuration Benefits

**1. Automatic Dependency Resolution**

Nx reads `package.json` to build dependency graph:

```json
// packages/eslint-plugin/package.json
{
  "dependencies": {
    "@forge-js/eslint-plugin-utils": "workspace:*"
    // â†‘ Nx detects this dependency
  }
}
```

**2. Caching Layer**

- Tests cached by project + input hash
- If inputs unchanged â†’ reuse cache
- JUnit reports included in cache output

**3. Parallel Execution**

```bash
# All independent projects run in parallel
pnpm nx affected -t test --parallel=4

# Timeline:
# [0-1s]  eslint-plugin-utils test âœ…
#         cli test âœ…
#         playground test âœ…
# [1-5s]  eslint-plugin test âœ… (waits for utils)
```

### Local Testing Tips

**Test only affected packages locally:**

```bash
# Test what changed
pnpm nx affected -t test

# Test specific branch against main
pnpm nx affected -t test --base=main

# Test including dependents
pnpm nx affected -t test --include-dependents
```

**Run all tests (like CI on main):**

```bash
# Full suite
pnpm nx run-many -t test --all --coverage

# With specific parallelism
pnpm nx run-many -t test --all --parallel=4
```

### CI Optimization Results

**Before (always full test suite):**
- Every PR: 5-10 minutes
- 100% coverage every time
- Expensive CI minutes

**After (affected on PR, full on main):**
- Small PRs: 30-60 seconds (15x faster!) ğŸš€
- Full commits: 5-10 minutes (complete coverage)
- Better resource utilization

### Troubleshooting `nx affected`

**Issue:** `nx affected` runs too many packages

```bash
# Solution: Verify dependency graph
pnpm nx graph

# Check what's affected
pnpm nx affected --verbose
```

**Issue:** Changes not detected

```bash
# Solution: Ensure git history available
git fetch origin main
pnpm nx affected -t test --base=origin/main
```

**Issue:** Need to force full test suite

```bash
# Override: Force all tests
pnpm nx run-many -t test --all --no-cache
```

### Future Enhancements

Potential improvements to dependency graph:

1. **Cross-package type checking**
   - `dependsOn: ["typecheck"]` for type safety

2. **Integration test layer**
   - Separate target for end-to-end tests
   - Run only after unit tests pass

3. **Conditional test targets**
   - Skip tests for documentation-only changes
   - Pattern: `ignorePattern` in project.json

---

## ğŸ¯ Codecov Components + JUnit Test Analytics Integration

### How They Work Together

Codecov uses **two complementary systems** for complete test insights:

```mermaid
%%{init: {
  'theme': 'base',
  'themeVariables': {
    'primaryColor': '#f8fafc',
    'primaryTextColor': '#1e293b',
    'primaryBorderColor': '#334155',
    'lineColor': '#475569'
  }
}}%%
graph LR
    A["ğŸ“Š Code Coverage<br/>(coverage-final.json)"] --> C["âœ… Component Status<br/>in PR Comments"]
    B["ğŸ§ª Test Analytics<br/>(test-report.junit.xml)"] --> C
    C --> D["Codecov Dashboard"]
    
    classDef coverage fill:#d1fae5,stroke:#059669,stroke-width:2px
    classDef tests fill:#dbeafe,stroke:#2563eb,stroke-width:2px
    classDef dashboard fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
    
    class A coverage
    class B tests
    class D dashboard
```

### Coverage Report vs Test Report

| Aspect | Coverage Report (JSON) | Test Report (JUnit XML) |
|--------|------------------------|-------------------------|
| **What it measures** | % of code executed by tests | Individual test results |
| **Format** | `coverage-final.json` | `test-report.junit.xml` |
| **Component tracking** | âœ… Per-component coverage % | âœ… Per-component test results |
| **Used for** | Coverage thresholds | Test analytics dashboard |
| **Codecov feature** | Coverage components | Test analytics (beta) |
| **CLI command** | `vitest run --coverage` | `vitest run --reporter=junit` |

### Our Configuration

**Each package generates BOTH:**

```
packages/eslint-plugin/
â”œâ”€â”€ coverage/
â”‚   â””â”€â”€ coverage-final.json          â† Code coverage (85% target)
â””â”€â”€ test-report.junit.xml             â† Test execution results

packages/cli/
â”œâ”€â”€ coverage/
â”‚   â””â”€â”€ coverage-final.json          â† Code coverage (75% target)
â””â”€â”€ test-report.junit.xml             â† Test execution results

packages/eslint-plugin-utils/
â”œâ”€â”€ coverage/
â”‚   â””â”€â”€ coverage-final.json          â† Code coverage (80% target)
â””â”€â”€ test-report.junit.xml             â† Test execution results

apps/playground/
â”œâ”€â”€ coverage/
â”‚   â””â”€â”€ coverage-final.json          â† Code coverage (60% target)
â””â”€â”€ test-report.junit.xml             â† Test execution results
```

### CI Workflow Integration

Our workflow uploads **both reports separately** with **different flags**:

```yaml
# Step 1: Collect both types of reports
- name: Collect coverage files
  run: find ./packages ./apps -name "coverage-final.json" ...

- name: Collect JUnit test reports
  run: find ./packages ./apps -name "test-report.junit.xml" ...

# Step 2: Upload coverage (for component statuses)
- name: Upload coverage to Codecov
  uses: codecov/codecov-action@v5
  with:
    files: ${{ steps.coverage.outputs.files }}
    flags: 'pr'  # â† Different flag for coverage

# Step 3: Upload test analytics (for test insights)
- name: Upload test analytics to Codecov
  uses: codecov/codecov-action@v5
  with:
    files: ${{ steps.tests.outputs.files }}
    flags: 'test-pr'  # â† Different flag for tests
```

### Codecov Components Track Both

Your `codecov.yml` components now get insights from BOTH sources:

```yaml
individual_components:
  - component_id: eslint_plugin
    name: "ESLint Plugin"
    paths:
      - "packages/eslint-plugin/**"
    statuses:
      # Tracks from coverage-final.json
      - type: project
        target: 85   # Must reach 85% coverage
      # Automatically enhanced with test analytics
      # from test-report.junit.xml
```

**What Codecov does:**
1. âœ… Matches files in `coverage-final.json` to component paths
2. âœ… Matches test files in `test-report.junit.xml` to component paths
3. âœ… Creates per-component coverage status checks
4. âœ… Shows test execution details in dashboard
5. âœ… Tracks test performance and flaky tests

### What You'll See in PR Comments

Codecov now shows enhanced component information:

```
ğŸ“Š Coverage Report

Overall: 75.5% (+1.2% from main)

ğŸ“¦ Components:
â”œâ”€ ESLint Plugin: 92% âœ… (target 85%)
â”‚  â””â”€ 81 tests passed in 750ms
â”œâ”€ CLI: 74% âš ï¸ (target 75%, -1%)
â”‚  â””â”€ 0 tests (needs tests!)
â”œâ”€ Security Rules: 95% âœ… (target 90%)
â”‚  â””â”€ 16 tests passed in 320ms
â””â”€ Utils: 88% âœ… (target 80%)
   â””â”€ 12 tests passed in 280ms
```

### Dashboard Features Unlocked

With JUnit reports, Codecov dashboard now shows:

1. **Test Execution Times**
   - Which tests are slow
   - Performance trends

2. **Flaky Test Detection**
   - Tests that fail randomly
   - Reliability indicators

3. **Test Results by Component**
   - Pass/fail rates per package
   - Test failure patterns

4. **Coverage + Test Correlation**
   - Coverage vs test count
   - Identify under-tested code

### Local Testing

Verify both reports generate correctly:

```bash
# Run tests with both coverage and JUnit reporting
pnpm nx run-many -t test --all --coverage

# Check both report types were created
ls packages/*/coverage/coverage-final.json
ls packages/*/test-report.junit.xml

# View test report in browser (optional)
cat packages/eslint-plugin/test-report.junit.xml
```

### Troubleshooting

**Problem:** JUnit files not found in CI
```bash
# Solution: Verify reporters are configured
grep -r "reporters.*junit" packages/*/vitest.config.mts
```

**Problem:** Codecov not showing test analytics
```bash
# Solution: Ensure JUnit flag differs from coverage flag
# Coverage uses: flags: 'pr'
# Tests use: flags: 'test-pr'
```

**Problem:** Components not showing in Codecov dashboard
```bash
# Solution: Wait 5-10 minutes for Codecov to process both reports
# Check codecov.yml has components defined with matching paths
```

### Future Enhancements

Codecov Test Analytics (beta) eventually enables:
- ğŸ”´ **Fail gates:** Require tests for new code
- ğŸ“ˆ **Test trends:** Track test health over time
- ğŸ¯ **Test targets:** Set minimum test count per component
- ğŸ” **Flaky detection:** Automatically flag unreliable tests
